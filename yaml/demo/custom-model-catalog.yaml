kind: ConfigMap
apiVersion: v1
metadata:
  name: model-catalog-sources
  namespace: rhoai-model-registries
  labels:
    app: model-catalog
    app.kubernetes.io/component: model-catalog
    app.kubernetes.io/created-by: model-registry-operator
    app.kubernetes.io/instance: model-catalog
    app.kubernetes.io/managed-by: model-registry-operator
    app.kubernetes.io/name: model-catalog
    app.kubernetes.io/part-of: model-catalog
    component: model-catalog
  annotations:
    banzaicloud.com/last-applied: UEsDBBQACAAIAAAAAAAAAAAAAAAAAAAAAAAIAAAAb3JpZ2luYWyckU9LxEAMxb+KvHNnxetcPXv1Ih7SNluH7SRDkhXK0u8uq6sgVCjehsl775c/F1Arz2xeVJDx/oAOIwUhX+B6toH9sFCdkTFQ0KyT57uXV6wdTkVGZDyqHMv0RA0dKgd9u2fqefbri1pDRtWR53QLQXf9PZzOPZtwsB+K3g9amwpL7FMbU/CY+uVHbjwVD1uSNjYKtU1jEQ+SgXdRKglN/6EI1X2ERhZJjxvav/exdtjKT7eL4avsjT6ntDelkn41X9ixrh8BAAD//1BLBwjMoPM4ywAAAP0BAABQSwECFAAUAAgACAAAAAAAzKDzOMsAAAD9AQAACAAAAAAAAAAAAAAAAAAAAAAAb3JpZ2luYWxQSwUGAAAAAAEAAQA2AAAAAQEAAAAA
data:
  sample-catalog.yaml: |-
    source: Hugging Face
    models:
    - name: RedHatAI/Qwen3-Coder-480B-A35B-Instruct-FP8
      description: Qwen3-Coder-480B-A35B-Instruct is an open-source, mixture-of-experts (MoE) large language model developed by Alibaba Cloud's Qwen team specifically for agentic coding tasks. 
      readme: |-
        # Qwen3-Coder-480B-A35B-Instruct-FP8
        <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">
            <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>
        </a>

        ## Highlights

        Today, we're announcing **Qwen3-Coder**, our most agentic code model to date. **Qwen3-Coder** is available in multiple sizes, but we're excited to introduce its most powerful variant first: **Qwen3-Coder-480B-A35B-Instruct**. featuring the following key enhancements:  

        - **Significant Performance** among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks, achieving results comparable to Claude Sonnet.
        - **Long-context Capabilities** with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.
        - **Agentic Coding** supporting for most platform such as **Qwen Code**, **CLINE**, featuring a specially designed function call format.

        ![image/jpeg](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg)

        ## Model Overview

        **Qwen3-480B-A35B-Instruct** has the following features:
        - Type: Causal Language Models
        - Training Stage: Pretraining & Post-training
        - Number of Parameters: 480B in total and 35B activated
        - Number of Layers: 62
        - Number of Attention Heads (GQA): 96 for Q and 8 for KV
        - Number of Experts: 160
        - Number of Activated Experts: 8
        - Context Length: **262,144 natively**. 

        **NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**

        For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-coder/), [GitHub](https://github.com/QwenLM/Qwen3-Coder), and [Documentation](https://qwen.readthedocs.io/en/latest/).


        ## Quickstart

        We advise you to use the latest version of `transformers`.

        With `transformers<4.51.0`, you will encounter the following error:
        ```
        KeyError: 'qwen3_moe'
        ```

        The following contains a code snippet illustrating how to use the model generate content based on given inputs. 
        ```python
        from transformers import AutoModelForCausalLM, AutoTokenizer

        model_name = "Qwen/Qwen3-480B-A35B-Instruct"

        # load the tokenizer and the model
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto"
        )

        # prepare the model input
        prompt = "Write a quick sort algorithm."
        messages = [
            {"role": "user", "content": prompt}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # conduct text completion
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=65536
        )
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

        content = tokenizer.decode(output_ids, skip_special_tokens=True)

        print("content:", content)
        ```

        **Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**


        ## Note on FP8

        For convenience and performance, we have provided `fp8`-quantized model checkpoint for Qwen3, whose name ends with `-FP8`. The quantization method is fine-grained `fp8` quantization with block size of 128. You can find more details in the `quantization_config` field in `config.json`.

        You can use the Qwen3-480B-A35B-Instruct-FP8 model with serveral inference frameworks, including `transformers`, `sglang`, and `vllm`, as the original bfloat16 model.
        However, please pay attention to the following known issues:
        - `transformers`:
            - there are currently issues with the "fine-grained fp8" method in `transformers` for distributed inference. You may need to set the environment variable `CUDA_LAUNCH_BLOCKING=1` if multiple devices are used in inference.


        ## Agentic Coding

        Qwen3-Coder excels in tool calling capabilities. 

        You can simply define or use any tools as following example.
        ```python
        # Your tool implementation
        def square_the_number(num: float) -> dict:
            return num ** 2

        # Define Tools
        tools=[
            {
                "type":"function",
                "function":{
                    "name": "square_the_number",
                    "description": "output the square of the number.",
                    "parameters": {
                        "type": "object",
                        "required": ["input_num"],
                        "properties": {
                            'input_num': {
                                'type': 'number', 
                                'description': 'input_num is a number that will be squared'
                                }
                        },
                    }
                }
            }
        ]

        import OpenAI
        # Define LLM
        client = OpenAI(
            # Use a custom endpoint compatible with OpenAI API
            base_url='http://localhost:8000/v1',  # api_base
            api_key="EMPTY"
        )
        
        messages = [{'role': 'user', 'content': 'square the number 1024'}]

        completion = client.chat.completions.create(
            messages=messages,
            model="Qwen3-Coder-480B-A35B-Instruct",
            max_tokens=65536,
            tools=tools,
        )

        print(completion.choice[0])
        ```

        ## Best Practices

        To achieve optimal performance, we recommend the following settings:

        1. **Sampling Parameters**:
          - We suggest using `temperature=0.7`, `top_p=0.8`, `top_k=20`, `repetition_penalty=1.05`.

        2. **Adequate Output Length**: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.


        ### Citation

        If you find our work helpful, feel free to give us a cite.

        ```
        @misc{qwen3technicalreport,
              title={Qwen3 Technical Report}, 
              author={Qwen Team},
              year={2025},
              eprint={2505.09388},
              archivePrefix={arXiv},
              primaryClass={cs.CL},
              url={https://arxiv.org/abs/2505.09388}, 
        }
        ```      
      provider: Qwen
      logo: data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?><svg id="uuid-9f039e68-6b98-43e1-9f17-f09cde1b77ad" xmlns="http://www.w3.org/2000/svg" width="175" height="175" viewBox="0 0 175 175"><defs><style>.uuid-a6742c41-a427-43de-bc2e-b163eb8c33ff{fill:none;}.uuid-d8435829-9dba-4ac8-896e-1bcf23acda26{fill:#e0e0e0;}.uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8{fill:#fff;}.uuid-c4d1bbf0-6dfd-4211-bfcb-bb38b56ed74a{fill:#63993d;}.uuid-ef85c11f-b673-4ff0-a612-f0db0622cea2{fill:#87bb62;}.uuid-2dc5f78c-8e80-4156-8617-c99edbeddc8c{fill:#e00;}</style></defs><g id="uuid-3f04d3d9-1c27-4da6-ae75-1f9aa33ca35d"><g id="uuid-da38815a-020a-4708-bed7-3bda0bcfec4b"><rect class="uuid-c4d1bbf0-6dfd-4211-bfcb-bb38b56ed74a" x="122.5" y="122.5" width="50" height="50" rx="12.5" ry="12.5"/><path class="uuid-ef85c11f-b673-4ff0-a612-f0db0622cea2" d="M160,123.5c6.34113,0,11.5,5.15888,11.5,11.5v25c0,6.34113-5.15887,11.5-11.5,11.5h-25c-6.34112,0-11.5-5.15887-11.5-11.5v-25c0-6.34112,5.15888-11.5,11.5-11.5h25M160,122.5h-25c-6.90356,0-12.5,5.59644-12.5,12.5v25c0,6.90356,5.59644,12.5,12.5,12.5h25c6.90356,0,12.5-5.59644,12.5-12.5v-25c0-6.90356-5.59644-12.5-12.5-12.5h0Z"/></g><g id="uuid-f039edac-873d-4ac0-8221-e5fbb7ba0063"><g id="uuid-0117ea0a-577f-4e22-bb9b-6b4160276746"><g id="uuid-4e9552fb-eaa1-42a1-ac08-124fbf5442dd"><rect class="uuid-a6742c41-a427-43de-bc2e-b163eb8c33ff" x="131.5" y="131.5" width="32" height="32"/></g><g id="uuid-6a05fd39-98e3-454f-96fd-fd57fbfe27d0"><path class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" d="M142.7,158.3672c-.61408,0-1.22976-.2328-1.6968-.7016l-7.4344-7.4344c-.62496-.62496-.62496-1.63744,0-2.26256s1.63744-.62496,2.26256,0l6.8688,6.8688,16.4688-16.4688c.62496-.62496,1.63744-.62496,2.26256,0,.62512.62496.62496,1.63744,0,2.26256l-17.0344,17.0344c-.4672.4688-1.08288.7016-1.6968.7016h-.00032Z"/></g></g></g></g><g id="uuid-c08babdc-111c-4221-85c8-90c8e21a7d59"><path class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" d="M117.5,135c0-9.64954,7.85046-17.5,17.5-17.5h17.40784c.0545-.82721.09216-1.659.09216-2.5V40c0-20.71069-16.78931-37.5-37.5-37.5H40C19.28931,2.5,2.5,19.28931,2.5,40v75c0,20.71063,16.78931,37.5,37.5,37.5h75c.841,0,1.67279-.03766,2.5-.09216v-17.40784Z"/><path class="uuid-d8435829-9dba-4ac8-896e-1bcf23acda26" d="M117.5,147.18536c-.82605.0636-1.6579.10626-2.5.10626H40c-17.80573,0-32.29169-14.48596-32.29169-32.29163V40C7.70831,22.19434,22.19427,7.70831,40,7.70831h75c17.80573,0,32.29169,14.48602,32.29169,32.29169v75c0,.8421-.04266,1.67395-.10626,2.5h5.22241c.0545-.82721.09216-1.659.09216-2.5V40c0-20.71075-16.78925-37.5-37.5-37.5H40C19.28925,2.5,2.5,19.28925,2.5,40v75c0,20.71069,16.78925,37.5,37.5,37.5h75c.841,0,1.67279-.03766,2.5-.09216v-5.22247Z"/></g><g id="uuid-b584108d-6227-400e-b197-b13065ec7f08"><rect class="uuid-a6742c41-a427-43de-bc2e-b163eb8c33ff" width="175" height="175"/><path class="uuid-2dc5f78c-8e80-4156-8617-c99edbeddc8c" d="M106.66691,109.27173h-58.33332c-1.05387,0-2.00195-.63477-2.40682-1.60726-.40283-.97249-.17904-2.09351.56559-2.83813l27.32544-27.32544-27.32544-27.32544c-.74463-.74463-.96842-1.86564-.56559-2.83813.40487-.97249,1.35295-1.60726,2.40682-1.60726h58.33332c1.05387,0,2.00195.63477,2.40682,1.60726.40283.97249.17904,2.09351-.56559,2.83813l-27.32544,27.32544,27.32544,27.32544c.74463.74463.96842,1.86564.56559,2.83813-.40487.97249-1.35295,1.60726-2.40682,1.60726ZM54.6202,104.0634h45.76008l-22.88004-22.88004-22.88004,22.88004ZM54.6202,50.93841l22.88004,22.88004,22.88004-22.88004h-45.76008Z"/><path class="uuid-2dc5f78c-8e80-4156-8617-c99edbeddc8c" d="M106.66691,80.10507h-58.33332c-1.43839,0-2.60417-1.16577-2.60417-2.60417s1.16577-2.60417,2.60417-2.60417h58.33332c1.43839,0,2.60417,1.16577,2.60417,2.60417s-1.16577,2.60417-2.60417,2.60417Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="102.5" y="102.49998" width="12.5" height="12.5"/><path d="M115.00024,117.60506h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM105.10441,112.39673h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="40.00001" y="102.49998" width="12.5" height="12.5"/><path d="M52.50025,117.60506h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM42.60441,112.39673h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="102.5" y="39.99999" width="12.5" height="12.5"/><path d="M115.00024,55.10507h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM105.10441,49.89674h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="40.00001" y="39.99999" width="12.5" height="12.5"/><path d="M52.50025,55.10507h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM42.60441,49.89674h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="40.00001" y="71.24999" width="12.5" height="12.5"/><path d="M52.50025,86.35507h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM42.60441,81.14673h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="102.5" y="71.24999" width="12.5" height="12.5"/><path d="M115.00024,86.35507h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM105.10441,81.14673h7.29167v-7.29167h-7.29167v7.29167Z"/><rect class="uuid-acb6f9ad-fab5-4e9f-9d5c-da7512f9afd8" x="71.25001" y="71.24999" width="12.5" height="12.5"/><path d="M83.75024,86.35507h-12.5c-1.43839,0-2.60417-1.16577-2.60417-2.60417v-12.5c0-1.43839,1.16577-2.60417,2.60417-2.60417h12.5c1.43839,0,2.60417,1.16577,2.60417,2.60417v12.5c0,1.43839-1.16577,2.60417-2.60417,2.60417ZM73.85441,81.14673h7.29167v-7.29167h-7.29167v7.29167Z"/></g></svg>
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0.txt
      libraryName: transformers
      artifacts:
        - uri: hf://RedHatAI/Qwen3-Coder-480B-A35B-Instruct-FP8
    - name: openai/gpt-oss-20b
      description: OpenAI's new gpt-oss models offer fast, low-cost, open-weight reasoning performance, strong tool use, customizability, and enterprise-ready safety for on-premise AI.
      readme: |-
        <p align="center">
          <img alt="gpt-oss-20b" src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg">
        </p>

        <p align="center">
          <a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> ·
          <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> ·
          <a href="https://arxiv.org/abs/2508.10925"><strong>Model card</strong></a> ·
          <a href="https://openai.com/index/introducing-gpt-oss/"><strong>OpenAI blog</strong></a>
        </p>

        <br>

        Welcome to the gpt-oss series, [OpenAI's open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.

        We're releasing two flavors of these open models:
        - `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)
        - `gpt-oss-20b` — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)

        Both models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.


        > [!NOTE]
        > This model card is dedicated to the smaller `gpt-oss-20b` model. Check out [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) for the larger model.

        # Highlights

        * **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.  
        * **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  
        * **Full chain-of-thought:** Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs. It’s not intended to be shown to end users.  
        * **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.
        * **Agentic capabilities:** Use the models’ native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.
        * **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.

        ---

        # Inference examples

        ## Transformers

        You can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.

        To get started, install the necessary dependencies to setup your environment:

        ```
        pip install -U transformers kernels torch 
        ```

        Once, setup you can proceed to run the model by running the snippet below:

        ```py
        from transformers import pipeline
        import torch

        model_id = "openai/gpt-oss-20b"

        pipe = pipeline(
            "text-generation",
            model=model_id,
            torch_dtype="auto",
            device_map="auto",
        )

        messages = [
            {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},
        ]

        outputs = pipe(
            messages,
            max_new_tokens=256,
        )
        print(outputs[0]["generated_text"][-1])
        ```

        Alternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:

        ```
        transformers serve
        transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b
        ```

        [Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)

        ## vLLM

        vLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.

        ```bash
        uv pip install --pre vllm==0.10.1+gptoss \
            --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
            --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
            --index-strategy unsafe-best-match

        vllm serve openai/gpt-oss-20b
        ```

        [Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)

        ## PyTorch / Triton

        To learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).

        ## Ollama

        If you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).

        ```bash
        # gpt-oss-20b
        ollama pull gpt-oss:20b
        ollama run gpt-oss:20b
        ```

        [Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)

        #### LM Studio

        If you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.

        ```bash
        # gpt-oss-20b
        lms get openai/gpt-oss-20b
        ```

        Check out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.

        ---

        # Download the model

        You can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:

        ```shell
        # gpt-oss-20b
        huggingface-cli download openai/gpt-oss-20b --include "original/*" --local-dir gpt-oss-20b/
        pip install gpt-oss
        python -m gpt_oss.chat model/
        ```

        # Reasoning levels

        You can adjust the reasoning level that suits your task across three levels:

        * **Low:** Fast responses for general dialogue.  
        * **Medium:** Balanced speed and detail.  
        * **High:** Deep and detailed analysis.

        The reasoning level can be set in the system prompts, e.g., "Reasoning: high".

        # Tool use

        The gpt-oss models are excellent for:
        * Web browsing (using built-in browsing tools)
        * Function calling with defined schemas
        * Agentic operations like browser tasks

        # Fine-tuning

        Both gpt-oss models can be fine-tuned for a variety of specialized use cases.

        This smaller model `gpt-oss-20b` can be fine-tuned on consumer hardware, whereas the larger [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) can be fine-tuned on a single H100 node.

        # Citation

        ```bibtex
        @misc{openai2025gptoss120bgptoss20bmodel,
              title={gpt-oss-120b & gpt-oss-20b Model Card}, 
              author={OpenAI},
              year={2025},
              eprint={2508.10925},
              archivePrefix={arXiv},
              primaryClass={cs.CL},
              url={https://arxiv.org/abs/2508.10925}, 
        }
        ```
      provider: Open AI
      logo: data:image/png;base64,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
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0.txt
      libraryName: transformers
      artifacts:
        - uri: oci://registry.redhat.io/rhelai1/modelcar-gpt-oss-20b:1.5
    - name: mistralai/Mistral-7B-Instruct-v0.3
      description: The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.
      readme: |-
        # Model Card for Mistral-7B-Instruct-v0.3

        The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.

        Mistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)
        - Extended vocabulary to 32768
        - Supports v3 Tokenizer
        - Supports function calling

        ## Installation

        It is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.

        ```
        pip install mistral_inference
        ```

        ## Download

        ```py
        from huggingface_hub import snapshot_download
        from pathlib import Path

        mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')
        mistral_models_path.mkdir(parents=True, exist_ok=True)

        snapshot_download(repo_id="mistralai/Mistral-7B-Instruct-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)
        ```

        ### Chat

        After installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using

        ```
        mistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256
        ```

        ### Instruct following

        ```py
        from mistral_inference.transformer import Transformer
        from mistral_inference.generate import generate

        from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
        from mistral_common.protocol.instruct.messages import UserMessage
        from mistral_common.protocol.instruct.request import ChatCompletionRequest


        tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
        model = Transformer.from_folder(mistral_models_path)

        completion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])

        tokens = tokenizer.encode_chat_completion(completion_request).tokens

        out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
        result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

        print(result)
        ```

        ### Function calling

        ```py
        from mistral_common.protocol.instruct.tool_calls import Function, Tool
        from mistral_inference.transformer import Transformer
        from mistral_inference.generate import generate

        from mistral_common.tokens.tokenizers.mistral import MistralTokenizer
        from mistral_common.protocol.instruct.messages import UserMessage
        from mistral_common.protocol.instruct.request import ChatCompletionRequest


        tokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tokenizer.model.v3")
        model = Transformer.from_folder(mistral_models_path)

        completion_request = ChatCompletionRequest(
            tools=[
                Tool(
                    function=Function(
                        name="get_current_weather",
                        description="Get the current weather",
                        parameters={
                            "type": "object",
                            "properties": {
                                "location": {
                                    "type": "string",
                                    "description": "The city and state, e.g. San Francisco, CA",
                                },
                                "format": {
                                    "type": "string",
                                    "enum": ["celsius", "fahrenheit"],
                                    "description": "The temperature unit to use. Infer this from the users location.",
                                },
                            },
                            "required": ["location", "format"],
                        },
                    )
                )
            ],
            messages=[
                UserMessage(content="What's the weather like today in Paris?"),
                ],
        )

        tokens = tokenizer.encode_chat_completion(completion_request).tokens

        out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)
        result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])

        print(result)
        ```

        ## Generate with `transformers`

        If you want to use Hugging Face `transformers` to generate text, you can do something like this.

        ```py
        from transformers import pipeline

        messages = [
            {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
            {"role": "user", "content": "Who are you?"},
        ]
        chatbot = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.3")
        chatbot(messages)
        ```


        ## Function calling with `transformers`

        To use this example, you'll need `transformers` version 4.42.0 or higher. Please see the 
        [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)
        in the `transformers` docs for more information.

        ```python
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        model_id = "mistralai/Mistral-7B-Instruct-v0.3"
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        def get_current_weather(location: str, format: str):
            """
            Get the current weather

            Args:
                location: The city and state, e.g. San Francisco, CA
                format: The temperature unit to use. Infer this from the users location. (choices: ["celsius", "fahrenheit"])
            """
            pass

        conversation = [{"role": "user", "content": "What's the weather like in Paris?"}]
        tools = [get_current_weather]


        # format and tokenize the tool use prompt 
        inputs = tokenizer.apply_chat_template(
                    conversation,
                    tools=tools,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt",
        )

        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

        inputs.to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=1000)
        print(tokenizer.decode(outputs[0], skip_special_tokens=True))
        ```

        Note that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool
        results to the chat history so that the model can use them in its next generation. For a full tool calling example, please
        see the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), 
        and note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be
        exactly 9 alphanumeric characters.


        ## Limitations

        The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. 
        It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to
        make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.

        ## The Mistral AI Team

        Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall
      provider: Mistral AI
      logo: data:image/png;base64,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
      license: apache-2.0
      licenseLink: https://www.apache.org/licenses/LICENSE-2.0.txt
      libraryName: transformers
      artifacts:
        - uri: hf://mistralai/Mistral-7B-Instruct-v0.3
  sources.yaml: |-
    catalogs:
    - name: Sample Catalog
      id: sample_custom_catalog
      type: yaml
      enabled: true
      properties:
        yamlCatalogPath: sample-catalog.yaml
